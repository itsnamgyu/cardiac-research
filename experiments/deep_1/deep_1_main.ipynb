{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import core.history as ch\n",
    "import core.fine_model as cm\n",
    "from core.fine_model import FineModel\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import cr_interface as cri\n",
    "import keras_utils as ku\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_by_t_epochs(fm, lr, decay, train_gen, val_gen, key, t=10):\n",
    "    fm.load_weights(key, verbose=0)\n",
    "    fm.compile_model(lr=lr, decay=decay)\n",
    "    result = fm.get_model().fit_generator(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=math.ceil(train_gen.n / BATCH_SIZE),\n",
    "        validation_steps=1,\n",
    "        shuffle=True,\n",
    "        epochs=t)\n",
    "    ch.append_history(result.history, fm.get_name(), key)\n",
    "    fm.save_weights(key, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k(train_collection, fm, lr, decay, k=5):\n",
    "    '''\n",
    "    Returns (loss, epochs)\n",
    "    '''\n",
    "    print('Converging {} [lr={}, decay={}]'.format(fm.get_name(), lr, decay).center(100, '-'))\n",
    "    \n",
    "    k_collections = train_collection.k_split(k)\n",
    "    _key = 'K{:02d}'\n",
    "    _train_dir = 'temp_images/train_' + _key\n",
    "    _val_dir = 'temp_images/val_' + _key\n",
    "    \n",
    "    aug_gen = fm.get_image_data_generator(augment=True)\n",
    "    pure_gen = fm.get_image_data_generator(augment=False)\n",
    "    \n",
    "    train_gens = []\n",
    "    val_gens = []\n",
    "    \n",
    "    print('Exporting images... ', end='')\n",
    "    for i, collection in enumerate(k_collections):\n",
    "        for j in range(5):\n",
    "            train_dir = _train_dir.format(j)\n",
    "            val_dir = _val_dir.format(j)\n",
    "            if i == j:\n",
    "                collection.export_by_label(val_dir)\n",
    "            else:\n",
    "                collection.export_by_label(train_dir)\n",
    "    print('complete.', end='')\n",
    "                \n",
    "    for i in range(k):\n",
    "        train_gens.append(aug_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=fm.get_output_shape(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical'\n",
    "        ))\n",
    "\n",
    "        val_gens.append(aug_gen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=fm.get_output_shape(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical'\n",
    "        ))\n",
    "                         \n",
    "    for i in range(k):\n",
    "        fm.save_weights(_key.format(i), verbose=0)\n",
    "        \n",
    "    while True:\n",
    "        print('Training 5 epochs for k splits... ', end='')\n",
    "        histories = []\n",
    "        for i in range(k):\n",
    "            key = _key.format(i)\n",
    "            train_k_by_t_epochs(fm, lr, decay, train_gens[i], val_gens[i], key)\n",
    "            histories.append(ch.load_history(fm.get_name(), key))\n",
    "        history = ch.get_average(histories)\n",
    "        index, value = ch.get_early_stop_index_and_value(history)\n",
    "        if index is not None:\n",
    "            print('converged!.')\n",
    "            print('Validation Loss: {}, Epoch {}'.format(value, index))\n",
    "            return history.loc[index, 'val_loss'], index\n",
    "        print('converging...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(train_collection, fm, lr=(1e-4, 1e-1), decay=(0, 1e-3), init_points=5, n_iter=5):\n",
    "    '''\n",
    "    lr, decay: (lower_bound, upper_bound)\n",
    "    Returns (lr, decay, epochs)\n",
    "    '''\n",
    "    print('Optimizing {}'.format(fm.get_name()))\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    def f(lr, decay):\n",
    "        loss, epochs = train_k(train_collection, fm, lr, decay)\n",
    "        results['lr'].append(lr)\n",
    "        results['decay'].append(decay)\n",
    "        results['loss'].append(loss)\n",
    "        results['epochs'].append(epochs)\n",
    "        return -loss\n",
    "    \n",
    "    pbounds = {\n",
    "        'lr': lr,\n",
    "        'decay': decay\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=f, pbounds=pbounds, random_state=1)\n",
    "    optimizer.maximize(init_points, n_iter)\n",
    "    \n",
    "    loss = -optimizer.max['target']\n",
    "    lr = optimizer.max['params']['lr']\n",
    "    decay = optimizer.max['params']['decay']\n",
    "    \n",
    "    print('Validation Loss of {} for [lr={}, decay={}]'.format(loss, lr, decay))\n",
    "    \n",
    "    for i in range(len(results['lr'])):\n",
    "        if results['decay'][i] == decay and results['lr'][i] == lr:\n",
    "            epochs = results['epochs'][i]\n",
    "            \n",
    "    return lr, decay, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_full_model(train_collection, test_collection, fm):\n",
    "    print('FULLY TRAINING {}'.format(fm.get_name().upper()).center(100, '='))\n",
    "    \n",
    "    aug_gen = fm.get_image_data_generator(augment=True)\n",
    "    pure_gen = fm.get_image_data_generator(augment=False)\n",
    "    \n",
    "    train_dir = 'temp_images/train'\n",
    "    test_dir = 'temp_images/test'\n",
    "    train_collection.export_by_label(train_dir)\n",
    "    test_collection.export_by_label(test_dir)\n",
    "    train_gen = aug_gen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=fm.get_output_shape(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    test_gen = pure_gen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=fm.get_output_shape(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    fm.reload_model()\n",
    "    \n",
    "    for d in range(len(fm.depths)):\n",
    "        print('TRAINING DEPTH #{}'.format(d).center(100, '='))\n",
    "        key = 'D{:02d}'.format(d)\n",
    "        fm.save_weights(key, verbose=0)\n",
    "        fm.set_depth(d)\n",
    "        \n",
    "        lr, decay, epochs = optimize_hyperparameters(train_collection, fm)\n",
    "        with open('hp_{}_d{}'.format(fm.get_name(), d)) as f:\n",
    "            f.write('lr,decay,epochs\\n')\n",
    "            f.write('{},{},{}\\n'.format(lr, decay, epochs))\n",
    "        \n",
    "        fm.load_weights(key, verbose=0)\n",
    "        result = fm.get_model().fit_generator(\n",
    "            train_gen,\n",
    "            steps_per_epoch=math.ceil(train_gen.n / BATCH_SIZE),\n",
    "            shuffle=True,\n",
    "            epochs=t)\n",
    "        \n",
    "        evaluation = fm.get_model().evaulate_generator(test_gen)\n",
    "        fm.save_weights(key, verbose=0)\n",
    "        \n",
    "        print('DEPTH #{} TRAIN RESULTS'.format(d))\n",
    "        print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_all_models(train_collection, test_collection):\n",
    "    for fm_class in FineModel.get_list():\n",
    "        optimize_full_model(train_collection, test_collection, fm_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cri.CrCollection.load().filter_by(dataset_index=0).tri_label().labeled()\n",
    "test = cri.CrCollection.load().filter_by(dataset_index=1).tri_label().labeled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = FineModel.get_dict()\n",
    "models.keys()\n",
    "#dict_keys(['xception', 'mobileneta25', 'mobilenetv2a35', 'vgg16', 'resnet50v2',\n",
    "#'inception_v3','inception_resnet_v2', 'densenet121', 'nasnet_mobile'])\n",
    "fm = models['mobileneta25']()\n",
    "optimize_full_model(train, test, fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_all_models(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
