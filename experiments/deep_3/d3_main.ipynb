{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')  # don't display mpl windows (will cause error in non-gui environment)\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "import keras\n",
    "\n",
    "import core.history as ch\n",
    "import core.fine_model as cm\n",
    "from core.fine_model import FineModel\n",
    "\n",
    "import cr_interface as cri\n",
    "import keras_utils as ku\n",
    "import analysis\n",
    "from lib import Timer, notify\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# number of folds\n",
    "K = 5\n",
    "\n",
    "# save intermediate weights\n",
    "SAVE_ALL_WEIGHTS = True\n",
    "\n",
    "# interval for saving intermediate weights (in epochs)\n",
    "T = 10\n",
    "\n",
    "# multiplier for out_of_myocardial (OAP, OBS) slices\n",
    "BALANCE = 5\n",
    "\n",
    "LEARNING_RATES = [\n",
    "    0.01, 0.00001\n",
    "]\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# experiment index to track saved model weights, training history etc.\n",
    "# iterate this index for each run (make sure to keep track of this index)\n",
    "EXP = 98\n",
    "\n",
    "# whether to sample 10% of all slices (for sanity checking purposes)\n",
    "SAMPLE = False\n",
    "\n",
    "# seed for k-fold split\n",
    "K_SPLIT_SEED = 1\n",
    "\n",
    "# models to train\n",
    "MODEL_KEYS = [\n",
    "    #'xception',\n",
    "    'mobileneta25',\n",
    "    #'mobilenetv2a35',\n",
    "    #'vgg16',\n",
    "    #'resnet50v2',\n",
    "    #'inception_v3',\n",
    "    #'inception_resnet_v2',\n",
    "    #'densenet121',\n",
    "    #'nasnet_mobile',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_learning_rate(fm: FineModel, depth_index, train_gens, val_gens, test_gen):\n",
    "    \"\"\"\n",
    "    Train the fine model (frozen at some given depth) for all five folds of data,\n",
    "    and choose the optimal learning rate BASED ON THE FINAL VALIDATION ACCURACY.\n",
    "    Consider learning rates defined in the global variable LEARNING_RATES\n",
    "    \n",
    "    \n",
    "    Save model with the following KEYS: [load weights via fm.load_weights(KEY)]\n",
    "    EXP01_D01\n",
    "    Fully trained model for the optimal learning rate\n",
    "    \n",
    "    \n",
    "    :param fm:\n",
    "    FineModel to train, i.e., the base network to train on\n",
    "    \n",
    "    :param depth_index:\n",
    "    The INDEX of the \"freeze depth\" for the given FineModel\n",
    "    \n",
    "    :param train_gens\n",
    "    List of train ImageDataGenerators for each fold\n",
    "    \n",
    "    :param val_gens  \n",
    "    List of validation ImageDataGenerators for each fold\n",
    "    \n",
    "    :param val_gens  \n",
    "    Test ImageDataGenerator for each fold\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def train_model_all_folds(fm, depth_index, lr_index,\n",
    "                          epochs, train_gens, val_gens, test_gen):\n",
    "    \"\"\"\n",
    "    Train the model (frozen at some depth) for all five folds\n",
    "\n",
    "\n",
    "    Saves intermediate models with the following KEYS: [load weights via fm.load_weights(KEY)]\n",
    "    EXP01_D01_L03_F01:\n",
    "    Fully trained model for the 1st freeze depth, 3rd learning rate, fold 1\n",
    "    EXP01_D01_L03_F01_E025:\n",
    "    Partially trained model for the 1st freeze depth, 3rd learning rate, fold 1, until the 25th epoch\n",
    "\n",
    "    Saves training history with the following KEYS: [get data via ch.get_history(model_name, KEY)]\n",
    "    EXP01_D01_L03_F01:\n",
    "    Training history for the 1st freeze depth, 3rd learning rate, fold 1\n",
    "\n",
    "\n",
    "    :param fm:\n",
    "    FineModel to train, i.e., the base network to train on\n",
    "\n",
    "    :param depth_index:\n",
    "    The INDEX of the \"freeze depth\" for the given FineModel\n",
    "\n",
    "    :param lr_index:\n",
    "    The INDEX of the learning rate, i.e., lr = LEARNING_RATES[lr_index]\n",
    "\n",
    "    :param epochs:\n",
    "    Number of epochs to train. MUST BE MULTIPLE OF 5.\n",
    "\n",
    "    :param train_gens\n",
    "    List of train ImageDataGenerators for each fold\n",
    "\n",
    "    :param val_gens\n",
    "    List of validation ImageDataGenerators for each fold\n",
    "\n",
    "    :param val_gens\n",
    "    Test ImageDataGenerator for each fold\n",
    "\n",
    "    :return:\n",
    "    tuple(val_loss, val_acc): AVERAGE validation loss and accuracy at FINAL EPOCH\n",
    "    \"\"\"\n",
    "    _exp_key = 'EXP{:02}'.format(EXP)\n",
    "    _depth_key = _exp_key + '_D{:02}'\n",
    "    _fold_key = _depth_key + '_L{:02}_F{:02}'\n",
    "    _epoch_key = _fold_key + '_E{:03}'\n",
    "\n",
    "    lr = LEARNING_RATES[lr_index]\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    # train the model K times, one for each fold\n",
    "    for i in range(K):\n",
    "        # load model at previous state\n",
    "        previous_depth_index = depth_index - 1\n",
    "        if previous_depth_index < 0:\n",
    "            fm.reload_model()\n",
    "        else:\n",
    "            fm.load_weights(_depth_key.format(previous_depth_index))\n",
    "        fm.set_depth(depth_index)\n",
    "        fm.compile_model(lr=lr)\n",
    "        model = fm.get_model()\n",
    "\n",
    "        print('[debug] batch: {}'.format(BATCH_SIZE))\n",
    "        print('[debug] size: {}'.format(train_gens[i].n))\n",
    "        print('[debug] steps: {}'.format(len(train_gens[i])))\n",
    "\n",
    "        # train T epochs at a time\n",
    "        start_epoch = 0\n",
    "        save_interval = T\n",
    "        while start_epoch < epochs:\n",
    "            print('[debug] epoch {}'.format(start_epoch))\n",
    "            target_epoch = start_epoch + save_interval\n",
    "            if target_epoch > epochs:\n",
    "                target_epoch = epochs\n",
    "            result = model.fit_generator(\n",
    "                train_gens[i],\n",
    "                validation_data=val_gens[i],\n",
    "                steps_per_epoch=len(train_gens[i]),\n",
    "                validation_steps=len(val_gens[i]),\n",
    "                workers=16,\n",
    "                use_multiprocessing=True,\n",
    "                shuffle=True,\n",
    "                epochs=target_epoch,\n",
    "                initial_epoch=start_epoch,\n",
    "            )\n",
    "            start_epoch = target_epoch\n",
    "\n",
    "            # update training history\n",
    "            ch.append_history(result.history, fm.get_name(), _fold_key.format(\n",
    "                depth_index, lr_index, i\n",
    "            ))\n",
    "            \n",
    "            if SAVE_ALL_WEIGHTS:\n",
    "                # save intermediate weights\n",
    "                fm.save_weights(_epoch_key.format(\n",
    "                    depth_index, lr_index, i, target_epoch,\n",
    "                ))\n",
    "\n",
    "        # save final weights\n",
    "        fm.save_weights(_fold_key.format(\n",
    "            depth_index, lr_index, i\n",
    "        ))\n",
    "        \n",
    "        print('[debug] test size: {}'.format(test_gen.n))\n",
    "        print('[debug] test steps: {}'.format(len(test_gen)))\n",
    "\n",
    "        loss, acc = model.evaluate_generator(\n",
    "            test_gen,\n",
    "            steps=len(test_gen),\n",
    "            #workers=4,\n",
    "            #use_multiprogressing=True,\n",
    "        )\n",
    "\n",
    "        print('[debug] test_loss={}, test_acc={}'.format(loss, acc))\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "    \n",
    "    print('Exporting analysis')\n",
    "    for metric in analysis.metric_names.keys():\n",
    "        analysis.analyze_lr(fm, fm.get_name(), depth_index, lr_index, lr, metric, exp=EXP)\n",
    "\n",
    "    total_loss = 0\n",
    "    for loss in loss_list:\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / K\n",
    "\n",
    "    total_acc = 0\n",
    "    for acc in acc_list:\n",
    "        total_acc += acc\n",
    "    avg_acc = total_acc / K\n",
    "\n",
    "    print('[debug] avg_test_loss={}, avg_test_acc={}'.format(avg_loss, avg_acc))\n",
    "\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_stats(train, test, folds):\n",
    "    # Print stats for each train/test set\n",
    "    def print_stats(collection):\n",
    "        df = collection.df\n",
    "        print('{:<3} patients / {:<4} images'.format(df.pid.unique().shape[0], df.shape[0]))\n",
    "        print(df.label.value_counts().to_string())\n",
    "\n",
    "    print('Training/Validation Set'.center(80, '-'))\n",
    "    print_stats(train)\n",
    "\n",
    "    print('Test Set'.center(80, '-'))\n",
    "    print_stats(test)\n",
    "\n",
    "    print()\n",
    "    print('Note that OAP, OBS images in the training/validation set will be duplicated 5 times')\n",
    "    print('to solve the class imbalance issue')\n",
    "    print()\n",
    "\n",
    "    # Print number of images by fold by label (training data)\n",
    "    stats = dict()\n",
    "    for i, fold in enumerate(folds):\n",
    "        counts = fold.df.label.value_counts()\n",
    "        counts.loc['total'] = fold.df.shape[0]\n",
    "        stats[i + 1] = counts\n",
    "    stats = pd.DataFrame(stats)\n",
    "\n",
    "    print('5-Fold Training Set Data'.center(80, '-'))\n",
    "    print(stats.to_string(col_space=8))\n",
    "    print()\n",
    "\n",
    "    # Columnwise-print or cr_codes (training data)\n",
    "    cr_codes_by_fold = list(sorted(fold.df.pid.unique()) for fold in folds)\n",
    "    max_len = 0\n",
    "    for codes in cr_codes_by_fold:\n",
    "        if max_len < len(codes):\n",
    "            max_len = len(codes)\n",
    "    for i, _ in  enumerate(folds):\n",
    "        print('Fold {}'.format(i + 1).ljust(16), end='')\n",
    "    print()\n",
    "    print('-' * 80)\n",
    "    for i in range(max_len):\n",
    "        for codes in cr_codes_by_fold:\n",
    "            if i < len(codes):\n",
    "                print('{:<16d}'.format(codes[i]), end='')\n",
    "            else:\n",
    "                print('{:<16s}'.format(''), end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_model(model_key, train_folds, test):\n",
    "    print(' MODEL: {} '.format(model_key).center(100, '#'))\n",
    "    keras.backend.clear_session()\n",
    "    models = FineModel.get_dict()\n",
    "    fm = models[model_key]()\n",
    "    train_gens, val_gens = fm.get_train_val_generators(train_folds)\n",
    "    test_gen = fm.get_test_generator(test)\n",
    "    for i, lr in enumerate(LEARNING_RATES):\n",
    "        print('Starting training {} lr={}'.format(fm.get_name(), lr).center(100, '-'))\n",
    "        train_model_all_folds(fm, 0, i, EPOCHS, train_gens, val_gens, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = cri.CrCollection.load().filter_by(dataset_index=0).tri_label().labeled()\n",
    "    test = cri.CrCollection.load().filter_by(dataset_index=1).tri_label().labeled()\n",
    "    if SAMPLE:\n",
    "        train = train.sample(frac=0.1)\n",
    "        test = test.sample(frac=0.1)\n",
    "    folds = train.k_split(K, seed=K_SPLIT_SEED)\n",
    "    \n",
    "    print_all_stats(train, test, folds)\n",
    "    \n",
    "    for key in MODEL_KEYS:\n",
    "        run_on_model(key, folds, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main()\n",
    "except Exception as e:\n",
    "    error = traceback.format_exc()\n",
    "    error += '\\n'\n",
    "    error += str(e)\n",
    "    print(error)\n",
    "    notify(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
