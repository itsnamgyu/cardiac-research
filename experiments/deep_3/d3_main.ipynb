{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "import keras\n",
    "\n",
    "import core.history as ch\n",
    "import core.fine_model as cm\n",
    "from core.fine_model import FineModel\n",
    "\n",
    "import cr_interface as cri\n",
    "import keras_utils as ku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "K = 5\n",
    "BALANCE = 5\n",
    "LEARNING_RATES = [\n",
    "    0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001\n",
    "]\n",
    "EPOCHS = 3\n",
    "SAMPLE = True # sample 10% of examples for testing (sanity check stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_IMAGE_DIR = 'temp_image'\n",
    "\n",
    "\n",
    "def get_train_val_generators(fm: FineModel, folds):\n",
    "    \"\"\"\n",
    "    Get train/validation ImageDataGenerators for the given model for each fold.\n",
    "    Note that subsequent calls to this method will invalidate the generators\n",
    "    returned from previous calls.\n",
    "    \n",
    "    Train/validation images are BOTH BALANCED AND AUGMENTED\n",
    "    \n",
    "    :param fm: \n",
    "    The base model for which you want to use the generators\n",
    "    \n",
    "    :param folds: \n",
    "    \n",
    "    :return: \n",
    "    tuple(train_gens, val_gens)\n",
    "    \n",
    "    train_gens: list of ImageDataGenerators for the train data in each fold\n",
    "    val_gens: list of ImageDataGenerators for the validation data in each fold\n",
    "    \"\"\"\n",
    "    print('Loading Train/Val ImageDataGenerators'.center(80, '-'))\n",
    "    \n",
    "    aug_gen = fm.get_image_data_generator(augment=True)\n",
    "    \n",
    "    val_gens = []\n",
    "    train_gens = []\n",
    "    \n",
    "    for i in range(len(folds)):\n",
    "        val_dir = os.path.join(TEMP_IMAGE_DIR, 'val{}'.format(i))\n",
    "        train_dir = os.path.join(TEMP_IMAGE_DIR, 'train{}'.format(i))\n",
    "        \n",
    "        # refresh directories\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        shutil.rmtree(val_dir)\n",
    "        shutil.rmtree(train_dir)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "        fold: cri.CrCollection\n",
    "        for j, fold in enumerate(folds):\n",
    "            if i == j:\n",
    "                # export validation data for fold i\n",
    "                fold.export_by_label(val_dir, balancing=5)\n",
    "            else:\n",
    "                # export train data for fold i\n",
    "                fold.export_by_label(train_dir, balancing=5)\n",
    "        \n",
    "        train_gens.append(aug_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=fm.get_output_shape(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "        ))\n",
    "        val_gens.append(aug_gen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=fm.get_output_shape(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "        ))\n",
    "        \n",
    "        print('Fold {}: {:<4} train images / {:<4} validation images'.format(\n",
    "            i + 1, train_gens[-1].n, val_gens[-1].n,\n",
    "        ))\n",
    "    \n",
    "    test_dir = os.path.join(TEMP_IMAGE_DIR, 'test')\n",
    "    for fold in folds:\n",
    "        # export test data for all\n",
    "        fold.export_by_label(test_dir, balancing=1) \n",
    "    \n",
    "    return train_gens, val_gens\n",
    "\n",
    "\n",
    "def get_test_generator(fm: FineModel, test_collection: cri.CrCollection):\n",
    "    \"\"\"\n",
    "    Get ImageDataGenerator for the test data, compatible with the given model.\n",
    "    Note that subsequent calls to this method will invalidate the generator\n",
    "    returned from previous calls.\n",
    "    \n",
    "    Test images are NOT AUGMENTED NOR BALANCED\n",
    "    \n",
    "    :param fm: \n",
    "    The base model for which you want to use the generators\n",
    "    \n",
    "    :param test_collection:\n",
    "    CrCollection containing test data\n",
    "    \n",
    "    :return: \n",
    "    ImageDataGenerator\n",
    "    \"\"\"\n",
    "    print('Loading Test ImageDataGenerator'.center(80, '-'))\n",
    "    \n",
    "    pure_gen = fm.get_image_data_generator(augment=False)\n",
    "    test_dir = os.path.join(TEMP_IMAGE_DIR, 'test')\n",
    "    \n",
    "    # refresh directories\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    shutil.rmtree(test_dir)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    test_collection.export_by_label(test_dir, balancing=1)\n",
    "    \n",
    "    print('[debug] test image count: {}'.format(test_collection.df.shape[0]))\n",
    "    \n",
    "    test_gen = pure_gen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=fm.get_output_shape(),\n",
    "        batch_size=test_collection.df.shape[0],\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "    )\n",
    "    print('Test images: {}'.format(test_gen.n))\n",
    "    \n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_learning_rate(fm: FineModel, depth_index, train_gens, val_gens, test_gen):\n",
    "    \"\"\"\n",
    "    Train the fine model (frozen at some given depth) for all five folds of data,\n",
    "    and choose the optimal learning rate BASED ON THE FINAL VALIDATION ACCURACY.\n",
    "    Consider learning rates defined in the global variable LEARNING_RATES\n",
    "    \n",
    "    \n",
    "    Save model with the following KEYS: [load weights via fm.load_weights(KEY)]\n",
    "    EXP01_D01\n",
    "    Fully trained model for the optimal learning rate\n",
    "    \n",
    "    \n",
    "    :param fm:\n",
    "    FineModel to train, i.e., the base network to train on\n",
    "    \n",
    "    :param depth_index:\n",
    "    The INDEX of the \"freeze depth\" for the given FineModel\n",
    "    \n",
    "    :param train_gens\n",
    "    List of train ImageDataGenerators for each fold\n",
    "    \n",
    "    :param val_gens  \n",
    "    List of validation ImageDataGenerators for each fold\n",
    "    \n",
    "    :param val_gens  \n",
    "    Test ImageDataGenerator for each fold\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def train_model_all_folds(fm, depth_index, lr_index,\n",
    "                          epochs, train_gens, val_gens, test_gen):\n",
    "    \"\"\"\n",
    "    Train the model (frozen at some depth) for all five folds\n",
    "\n",
    "\n",
    "    Saves intermediate models with the following KEYS: [load weights via fm.load_weights(KEY)]\n",
    "    EXP01_D01_L03_F01:\n",
    "    Fully trained model for the 1st freeze depth, 3rd learning rate, fold 1\n",
    "    EXP01_D01_L03_F01_E025:\n",
    "    Partially trained model for the 1st freeze depth, 3rd learning rate, fold 1, until the 25th epoch\n",
    "\n",
    "    Saves training history with the following KEYS: [get data via ch.get_history(model_name, KEY)]\n",
    "    EXP01_D01_L03_F01:\n",
    "    Training history for the 1st freeze depth, 3rd learning rate, fold 1\n",
    "\n",
    "\n",
    "    :param fm:\n",
    "    FineModel to train, i.e., the base network to train on\n",
    "\n",
    "    :param depth_index:\n",
    "    The INDEX of the \"freeze depth\" for the given FineModel\n",
    "\n",
    "    :param lr_index:\n",
    "    The INDEX of the learning rate, i.e., lr = LEARNING_RATES[lr_index]\n",
    "\n",
    "    :param epochs:\n",
    "    Number of epochs to train. MUST BE MULTIPLE OF 5.\n",
    "\n",
    "    :param train_gens\n",
    "    List of train ImageDataGenerators for each fold\n",
    "\n",
    "    :param val_gens\n",
    "    List of validation ImageDataGenerators for each fold\n",
    "\n",
    "    :param val_gens\n",
    "    Test ImageDataGenerator for each fold\n",
    "\n",
    "    :return:\n",
    "    tuple(val_loss, val_acc): AVERAGE validation loss and accuracy at FINAL EPOCH\n",
    "    \"\"\"\n",
    "    _depth_key = 'EXP01_D{:02}'\n",
    "    _fold_key = 'EXP01_D{:02}_L{:02}_F{:02}'\n",
    "    _epoch_key = 'EXP01_D{:02}_L{:02}_F{:02}_E{:03}'\n",
    "\n",
    "    lr = LEARNING_RATES[lr_index]\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    # train the model K times, one for each fold\n",
    "    for i in range(K):\n",
    "        # load model at previous state\n",
    "        previous_depth_index = depth_index - 1\n",
    "        if previous_depth_index < 0:\n",
    "            fm.reload_model()\n",
    "        else:\n",
    "            fm.load_weights(_depth_key.format(previous_depth_index))\n",
    "        fm.set_depth(depth_index)\n",
    "        fm.compile_model(lr=lr)\n",
    "        model = fm.get_model()\n",
    "\n",
    "        print('[debug] batch: {}'.format(BATCH_SIZE))\n",
    "        print('[debug] size: {}'.format(train_gens[i].n))\n",
    "        print('[debug] steps: {}'.format(len(train_gens[i])))\n",
    "\n",
    "        # train 5 epochs at a time\n",
    "        T = 5  # model save interval in epochs\n",
    "        start_epoch = 0\n",
    "        while start_epoch < epochs:\n",
    "            print('[debug] epoch {}'.format(start_epoch))\n",
    "            target_epoch = start_epoch + T\n",
    "            if target_epoch > epochs:\n",
    "                target_epoch = epochs\n",
    "            result = model.fit_generator(\n",
    "                train_gens[i],\n",
    "                validation_data=val_gens[i],\n",
    "                steps_per_epoch=len(train_gens[i]),\n",
    "                validation_steps=len(val_gens[i]),\n",
    "                #workers=4,\n",
    "                #use_multiprocessing=True,\n",
    "                shuffle=True,\n",
    "                epochs=target_epoch,\n",
    "                initial_epoch=start_epoch,\n",
    "            )\n",
    "            start_epoch = target_epoch\n",
    "\n",
    "            # update training history\n",
    "            ch.append_history(result.history, fm.get_name(), _fold_key.format(\n",
    "                depth_index, lr_index, i\n",
    "            ))\n",
    "            # save intermediate weights\n",
    "            fm.save_weights(_epoch_key.format(\n",
    "                depth_index, lr_index, i, target_epoch,\n",
    "            ))\n",
    "\n",
    "        # save final weights\n",
    "        fm.save_weights(_fold_key.format(\n",
    "            depth_index, lr_index, i\n",
    "        ))\n",
    "        \n",
    "        print('[debug] test size: {}'.format(test_gen.n))\n",
    "        print('[debug] test steps: {}'.format(len(test_gen)))\n",
    "\n",
    "        loss, acc = model.evaluate_generator(\n",
    "            test_gen,\n",
    "            steps=len(test_gen),\n",
    "            #workers=4,\n",
    "            #use_multiprogressing=True,\n",
    "        )\n",
    "\n",
    "        print('[debug] test_loss={}, test_acc={}'.format(loss, acc))\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    total_loss = 0\n",
    "    for loss in loss_list:\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / K\n",
    "\n",
    "    total_acc = 0\n",
    "    for acc in acc_list:\n",
    "        total_acc += acc\n",
    "    avg_acc = total_acc / K\n",
    "\n",
    "    print('[debug] avg_test_loss={}, avg_test_acc={}'.format(avg_loss, avg_acc))\n",
    "\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select train/test data and print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = cri.CrCollection.load().filter_by(dataset_index=0).tri_label().labeled()\n",
    "test = cri.CrCollection.load().filter_by(dataset_index=1).tri_label().labeled()\n",
    "\n",
    "if SAMPLE:\n",
    "    train = train.sample(frac=0.1)\n",
    "    test = test.sample(frac=0.1)\n",
    "\n",
    "\n",
    "def print_stats(collection):\n",
    "    df = collection.df\n",
    "    print('{:<3} patients / {:<4} images'.format(df.pid.unique().shape[0], df.shape[0]))\n",
    "    print(df.label.value_counts().to_string())\n",
    "\n",
    "\n",
    "print('Training/Validation Set'.center(80, '-'))\n",
    "print_stats(train)\n",
    "\n",
    "print('Test Set'.center(80, '-'))\n",
    "print_stats(test)\n",
    "\n",
    "print()\n",
    "print('Note that OAP, OBS images in the training/validation set will be duplicated 5 times')\n",
    "print('to solve the class imbalance issue')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print statistics on 5-fold split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = train.k_split(K)\n",
    "\n",
    "stats = dict()\n",
    "for i, fold in enumerate(folds):\n",
    "    counts = fold.df.label.value_counts()\n",
    "    counts.loc['total'] = fold.df.shape[0]\n",
    "    stats[i + 1] = counts\n",
    "stats = pd.DataFrame(stats)\n",
    "    \n",
    "print('5-Fold Training Set Data'.center(80, '-'))\n",
    "print(stats.to_string(col_space=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = FineModel.get_dict()\n",
    "models.keys()\n",
    "#dict_keys(['xception', 'mobileneta25', 'mobilenetv2a35', 'vgg16', 'resnet50v2',\n",
    "#'inception_v3','inception_resnet_v2', 'densenet121', 'nasnet_mobile'])\n",
    "fm = models['mobileneta25']()\n",
    "\n",
    "train_gens, val_gens = get_train_val_generators(fm, folds)\n",
    "test_gen = get_test_generator(fm, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lr in enumerate(LEARNING_RATES):\n",
    "    print('Starting training @lr={}'.format(lr).center(100, '-'))\n",
    "    train_model_all_folds(fm, 0, i, EPOCHS, train_gens, val_gens, test_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
