{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras_applications import vgg16, vgg19, inception_v3, resnet50, mobilenet, mobilenet_v2, inception_resnet_v2, xception, densenet, nasnet\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bottlenecks_and_labels(model, image_shape, augment_factor, flow_kwargs={}):\n",
    "    '''\n",
    "    Self Explanatory\n",
    "    \n",
    "    Arguments\n",
    "    - model: keras.model.Model, usually a pre-existing model excluding top-layers,\n",
    "             with pre-trained weights\n",
    "    - image_shape: tuple(x, y)\n",
    "    - augment_factor: how many times to augment non-test images\n",
    "    \n",
    "    Returns a tuple of 2 elements\n",
    "    - bottlenecks: dict of bottleneck np.ndarrays, by dataset\n",
    "    - labels: dict of label np.ndarrays, by dataset\n",
    "    '''\n",
    "    \n",
    "    # define augmenations\n",
    "    transform_parameters = {\n",
    "        'zx': 0.6,\n",
    "        'zy': 0.6,\n",
    "    }\n",
    "    zoom_gen = ImageDataGenerator()\n",
    "    zoom = lambda x: zoom_gen.apply_transform(x, transform_parameters)\n",
    "    \n",
    "    aug_gens = dict()\n",
    "    aug_gens['train'] = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            fill_mode='nearest',\n",
    "            preprocessing_function=zoom)\n",
    "    aug_gens['validation'] = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            fill_mode='nearest',\n",
    "            preprocessing_function=zoom)\n",
    "    aug_gens['test'] = ImageDataGenerator(\n",
    "            preprocessing_function=zoom)\n",
    "    \n",
    "    # get generator per dataset\n",
    "    ordered_gens = dict()\n",
    "    kwargs = dict(\n",
    "        target_size=image_shape,\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    kwargs.update(flow_kwargs)\n",
    "    for key, aug_gen in aug_gens.items():\n",
    "        ordered_gens[key] = aug_gen.flow_from_directory(\n",
    "            '../data/data/{}'.format(key), **kwargs)\n",
    "    \n",
    "    # generate bottleneck labels after augmentation\n",
    "    labels = dict()\n",
    "    for key, gen in ordered_gens.items():\n",
    "        if key == 'test':\n",
    "            labels[key] = gen.classes\n",
    "        else:\n",
    "            labels[key] = np.tile(gen.classes, augment_factor)\n",
    "\n",
    "    # generate bottlenecks by dataset\n",
    "    kwargs = dict(\n",
    "        verbose=1,\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "    )\n",
    "\n",
    "    bottlenecks = dict()\n",
    "    for key, gen in ordered_gens.items():\n",
    "        print('Preparing {} bottlenecks'.format(key))\n",
    "        bottlenecks[key] = model.predict_generator(\n",
    "            gen, steps=len(gen) * augment_factor, **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    @staticmethod\n",
    "    def get_input_shape(image_shape):\n",
    "        '''\n",
    "        Get input shape of conv-nets based on keras backend settings\n",
    "\n",
    "        Returns\n",
    "        tuple(n1, n2, n3)\n",
    "        '''\n",
    "\n",
    "        if keras.backend.image_data_format() == 'channels_first':\n",
    "            return (3,) + image_shape \n",
    "        else:\n",
    "            return image_shape + (3,)\n",
    "        \n",
    "    def __init__(self, model_func, image_shape, name, codename):\n",
    "        self.model_func = model_func\n",
    "        self.image_shape = image_shape\n",
    "        self.name = name\n",
    "        self.codename = codename\n",
    "        self.model = None\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model == None:\n",
    "            print('loading {} model'.format(self.name))\n",
    "            self.model = self.model_func(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=Model.get_input_shape(self.image_shape))\n",
    "        return self.model\n",
    "    \n",
    "    def free_model(self):\n",
    "        self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    Model(mobilenet.MobileNet, (224, 224), 'mobilenet', 'MOB'),\n",
    "    Model(mobilenet_v2.MobileNetV2, (224, 224), 'mobilenetv2', 'MOB2'),\n",
    "    Model(inception_resnet_v2.InceptionResNetV2, (299, 299), 'inceptionresnetv2', 'INCRES2'),\n",
    "    Model(inception_v3.InceptionV3, (299, 299), 'inceptionv3', 'INC3'),\n",
    "    #(densenet.DenseNet, (224, 224), 'densenet'),\n",
    "    Model(nasnet.NASNet, (224, 224), 'nasnet', 'NAS'),\n",
    "    Model(resnet50.ResNet50, (224, 224), 'resnet50', 'RES'),\n",
    "    Model(vgg16.VGG16, (224, 224), 'vgg16', 'VGG16'),\n",
    "    Model(vgg19.VGG19, (224, 244), 'vgg19', 'VGG19'),\n",
    "    Model(xception.Xception, (299, 299), 'xception',' XC'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['test', 'train', 'validation']\n",
    "BOTTLENECK_DIR = 'bottlenecks'\n",
    "LABEL_DIR = 'labels'\n",
    "\n",
    "\n",
    "def load_bottlenecks(model: Model, flow_kwargs={}):\n",
    "    bottleneck_dir = os.path.join(BOTTLENECK_DIR, model.codename)\n",
    "    label_dir = os.path.join(LABEL_DIR, model.codename)\n",
    "    \n",
    "    try:\n",
    "        bottlenecks = dict()\n",
    "        labels = dict()\n",
    "        for dataset in datasets:\n",
    "            bottlenecks[dataset] = np.load(\n",
    "                open(os.path.join(bottleneck_dir, '{}.npy'.format(dataset)), 'rb'))\n",
    "            labels[dataset] = np.load(\n",
    "                open(os.path.join(label_dir, '{}.npy'.format(dataset)), 'rb'))\n",
    "        print('loaded existing bottlenecks')\n",
    "    except FileNotFoundError as e:\n",
    "        print('generating bottlenecks...')\n",
    "        bottlenecks, labels = generate_bottlenecks_and_labels(\n",
    "            model.get_model(), model.image_shape, augment_factor=5, flow_kwargs=flow_kwargs)\n",
    "        os.makedirs(bottleneck_dir, exist_ok=True)\n",
    "        for dataset, val in bottlenecks.items():\n",
    "            np.save(open(os.path.join(bottleneck_dir, '{}.npy'.format(dataset)), 'wb'), val)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "        for dataset, val in labels.items():\n",
    "            np.save(open(os.path.join(label_dir, '{}.npy'.format(dataset)), 'wb'), val)\n",
    "            \n",
    "    return bottlenecks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_report(model: Model, optimizer, epochs, train_name=None, visualize=False):\n",
    "    now = datetime.datetime.now().strftime('%y-%m-%d-%H-%M')\n",
    "    if train_name:\n",
    "        identifier = '{}_{}'.format(train_name, now)\n",
    "    else:\n",
    "        identifier = '{}_{}'.format(model.codename, now)\n",
    "        \n",
    "    print('loading bottlenecks')\n",
    "    bottlenecks, labels = load_bottlenecks(model)\n",
    "\n",
    "    print('initializing top model for {}'.format(model.name))\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=model.get_model().output_shape[1:]))\n",
    "    top_model.add(Dense(1024, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    '''\n",
    "    top_model.add(Dense(256, \n",
    "                        activation='relu',\n",
    "                        kernel_initializer='random_uniform',\n",
    "                        bias_initializer='zeros'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    '''\n",
    "    top_model.add(Dense(3, activation='softmax'))\n",
    "    top_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    # one-hot labels\n",
    "    hot_labels = dict()\n",
    "    for key, label_array in labels.items():\n",
    "        hot_labels[key] = to_categorical(label_array, num_classes=3)\n",
    "\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(identifier))\n",
    "    \n",
    "    print('training model')\n",
    "    # train model\n",
    "    batch_size = 16\n",
    "    if visualize:\n",
    "        callbacks=[tensorboard, PlotLossesKeras()]\n",
    "    else:\n",
    "        callbacks=[tensorboard]\n",
    "    top_model.fit(bottlenecks['train'], hot_labels['train'],\n",
    "                  validation_data=(bottlenecks['validation'], hot_labels['validation']),\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    print('evaluate model')\n",
    "    # evaluate model\n",
    "    results = top_model.evaluate(bottlenecks['test'], hot_labels['test'])\n",
    "\n",
    "    # save weights for model\n",
    "    top_model.save_weights('weights/{}.hdf5'.format(identifier))\n",
    "    \n",
    "    print(name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(lr=1.0e-4, momentum=0.9)\n",
    "train_model_and_report(models[3], optimizer, 25, train_name=None, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = models[0]\n",
    "train_name = 'ho'\n",
    "now = datetime.datetime.now().strftime('%y-%m-%d-%H-%M')\n",
    "if train_name:\n",
    "    identifier = '{}_{}'.format(train_name, now)\n",
    "else:\n",
    "    identifier = '{}_{}'.format(model.codename, now)\n",
    "\n",
    "print('loading bottlenecks')\n",
    "bottlenecks, labels = load_bottlenecks(model)\n",
    "\n",
    "print('initializing top model')\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.get_model().output_shape[1:]))\n",
    "top_model.add(Dense(1024, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(3, activation='softmax'))\n",
    "top_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# one-hot labels\n",
    "hot_labels = dict()\n",
    "for key, label_array in labels.items():\n",
    "    hot_labels[key] = to_categorical(label_array, num_classes=3)\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(identifier))\n",
    "\n",
    "print('training model')\n",
    "# train model\n",
    "batch_size = 16\n",
    "if visualize:\n",
    "    callbacks=[tensorboard, PlotLossesKeras()]\n",
    "else:\n",
    "    callbacks=[tensorboard]\n",
    "top_model.fit(bottlenecks['train'], hot_labels['train'],\n",
    "              validation_data=(bottlenecks['validation'], hot_labels['validation']),\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STOPPPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(identifier))\n",
    "\n",
    "# train model\n",
    "batch_size = 16\n",
    "if visualize:\n",
    "    callbacks=[tensorboard, PlotLossesKeras()]\n",
    "else:\n",
    "    callbacks=[tensorboard]\n",
    "\n",
    "if visualize:\n",
    "    callbacks=[tensorboard, PlotLossesKeras()]\n",
    "else:\n",
    "    callbacks=[tensorboard]\n",
    "    \n",
    "kwargs = dict(validation_data=(bottlenecks['validation'], hot_labels['validation']),\n",
    "                  epochs=100,\n",
    "                  batch_size=16,\n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_and_eval(mobilenet.MobileNet, (224, 224), 'mobilenet')\n",
    "for model, shape, name in models:\n",
    "    train_and_eval(model, shape, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Code...\n",
    "Two-layer tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pre_model = models[3].get_model()\n",
    "# inception resnet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_model.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "for layer in pre_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "for layer in pre_model.layers[-3:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=pre_model.output_shape[1:]))\n",
    "top_model.add(Dense(1024, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "new_model.add(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_model.load_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  #optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "IMAGE_SHAPE = models[2].image_shape\n",
    "\n",
    "transform_parameters = {\n",
    "    'zx': 0.6,\n",
    "    'zy': 0.6,\n",
    "}\n",
    "zoom_gen = ImageDataGenerator()\n",
    "zoom = lambda x: zoom_gen.apply_transform(x, transform_parameters)\n",
    "gen = ImageDataGenerator(\n",
    "        preprocessing_function=zoom)\n",
    "aug_gen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=zoom)\n",
    "aug_gen2 = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=zoom)\n",
    "\n",
    "train_image_generator = aug_gen.flow_from_directory(\n",
    "    '../data/data/train',\n",
    "    target_size=(IMAGE_SHAPE),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_image_generator = aug_gen2.flow_from_directory(\n",
    "    '../data/data/validation',\n",
    "    target_size=(IMAGE_SHAPE),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_image_generator = gen.flow_from_directory(\n",
    "    '../data/data/test',\n",
    "    target_size=(IMAGE_SHAPE),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 50\n",
    "callbacks=[PlotLossesKeras()]\n",
    "new_model.fit_generator(\n",
    "    train_image_generator,\n",
    "    epochs=epoch_count,\n",
    "    validation_data=validation_image_generator,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate_generator(test_image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model.predict_generator(train_image_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
